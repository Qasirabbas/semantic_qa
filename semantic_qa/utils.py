import re
import pandas as pd
import openpyxl
from typing import List, Dict, Optional
from django.core.files.uploadedfile import UploadedFile
from io import BytesIO
import logging

logger = logging.getLogger('semantic_qa')

def get_client_ip(request):
    """Get client IP address from request"""
    x_forwarded_for = request.META.get('HTTP_X_FORWARDED_FOR')
    if x_forwarded_for:
        ip = x_forwarded_for.split(',')[0]
    else:
        ip = request.META.get('REMOTE_ADDR')
    return ip

def clean_text(text: str) -> str:
    """Clean and normalize text"""
    if not text:
        return ""
    
    # Remove extra whitespace and normalize
    text = re.sub(r'\s+', ' ', str(text)).strip()
    
    # Remove special characters but keep basic punctuation
    #modify history
    # roar edit 2025-6-17 
    # text = re.sub(r'[^\w\s\-_.,!?()]', '', text)
    
    return text

def extract_keywords_from_text(text: str) -> str:
    """Extract keywords from question and answer for better searching - supports Chinese"""
    if not text:
        return ""
    
    # Detect if text contains Chinese characters
    def contains_chinese(text):
        return any('\u4e00' <= char <= '\u9fff' for char in text)
    
    has_chinese = contains_chinese(text)
    
    if has_chinese:
        # Chinese text processing
        # Extract Chinese words (sequences of Chinese characters)
        chinese_words = re.findall(r'[\u4e00-\u9fff]+', text)
        
        # Extract English words and numbers
        english_words = re.findall(r'[a-zA-Z0-9]+', text)
        
        # Common Chinese stop words
        chinese_stop_words = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 
            'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½',
            'è‡ªå·±', 'è¿™', 'é‚£', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'å“ªé‡Œ', 'ä»€ä¹ˆæ—¶å€™',
            'æ€æ ·', 'ä¸ºäº†', 'å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶å', 'ç°åœ¨', 'å·²ç»', 'å¯ä»¥',
            'åº”è¯¥', 'éœ€è¦', 'å¿…é¡»', 'èƒ½å¤Ÿ', 'æˆ–è€…', 'è¿˜æ˜¯', 'å¦‚æœ', 'è™½ç„¶', 'é—®', 'ç­”'
        }
        
        # Filter Chinese words
        filtered_chinese = [word for word in chinese_words 
                           if word not in chinese_stop_words and len(word) > 1]
        
        # Common English stop words
        english_stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 
            'of', 'with', 'by', 'how', 'what', 'where', 'when', 'why', 'is', 'are', 
            'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 
            'did', 'will', 'would', 'could', 'should', 'can', 'may', 'might', 'must',
            'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they'
        }
        
        # Filter English words
        filtered_english = [word.lower() for word in english_words 
                           if word.lower() not in english_stop_words and len(word) > 2]
        
        # Combine all keywords
        all_keywords = filtered_chinese + filtered_english
        
        # Remove duplicates while preserving order
        seen = set()
        unique_keywords = []
        for keyword in all_keywords:
            if keyword not in seen:
                seen.add(keyword)
                unique_keywords.append(keyword)
        
        return ' '.join(unique_keywords[:20])  # Limit to 20 keywords
    
    else:
        # English text processing (original logic)
        text = text.lower()
        
        # Remove common stop words
        stop_words = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 
            'of', 'with', 'by', 'how', 'what', 'where', 'when', 'why', 'is', 'are', 
            'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 
            'did', 'will', 'would', 'could', 'should', 'can', 'may', 'might', 'must',
            'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they'
        }
        
        # Extract words and filter
        words = re.findall(r'\b\w+\b', text)
        keywords = [word for word in words if word not in stop_words and len(word) > 2]
        
        # Remove duplicates while preserving order
        seen = set()
        unique_keywords = []
        for keyword in keywords:
            if keyword not in seen:
                seen.add(keyword)
                unique_keywords.append(keyword)
        
        return ' '.join(unique_keywords[:15])  # Limit to 15 keywords for English
def clean_image_url(url: str) -> str:
    """Clean and fix image URLs - ENHANCED VERSION"""
    if not url:
        return ""
    
    url = url.strip()
    
    # Fix common URL formatting issues
    
    # Case 1: Missing :// after protocol
    if 'httpsae01' in url:
        url = url.replace('httpsae01', 'https://ae01')
    elif 'httpae01' in url:
        url = url.replace('httpae01', 'http://ae01')
    elif 'httpswww' in url:
        url = url.replace('httpswww', 'https://www')
    elif 'httpwww' in url:
        url = url.replace('httpwww', 'http://www')
    
    # Case 2: Missing slash after domain - SPECIFIC FOR ALICDN
    if 'ae01.alicdn.comkf' in url:
        url = url.replace('ae01.alicdn.comkf', 'ae01.alicdn.com/kf/')
    
    # Case 3: General missing :// fix
    if url.startswith('https') and '://' not in url:
        url = url.replace('https', 'https://', 1)
    elif url.startswith('http') and '://' not in url:
        url = url.replace('http', 'http://', 1)
    
    # Case 4: URL doesn't start with protocol at all
    elif not url.startswith(('http://', 'https://')):
        if 'alicdn.com' in url or 'ae01' in url:
            url = 'https://' + url
        else:
            url = 'https://' + url
    
    return url

def parse_excel_file(file: UploadedFile) -> List[Dict]:
    """Parse Excel file and extract QA entries with multilingual support"""
    try:
        # Read Excel file
        if file.name.endswith('.xlsx'):
            df = pd.read_excel(file, engine='openpyxl')
        else:
            df = pd.read_excel(file, engine='xlrd')
        
        # Clean column names - remove extra spaces and normalize
        df.columns = df.columns.str.strip()
        
        # Print columns for debugging
        logger.info(f"Excel columns found: {list(df.columns)}")
        
        # Define multilingual column mappings
        column_mappings = {
            'SKU': {
                'en': ['sku', 'product_id', 'part_number', 'product id', 'part number', 'code', 'id'],
                'zh': ['sku', 'äº§å“ç¼–å·', 'äº§å“ä»£ç ', 'ç¼–å·', 'ä»£ç ', 'äº§å“å·', 'å‹å·', 'è´§å·'],
                'variations': ['sku', 'product_id', 'part_number', 'productid', 'partnumber']
            },
            'Question': {
                'en': ['question', 'q', 'query', 'problem', 'issue', 'ask'],
                'zh': ['é—®é¢˜', 'ç–‘é—®', 'è¯¢é—®', 'å†…å®¹', 'é¢˜ç›®', 'q', 'question'],
                'variations': ['question', 'q', 'query', 'problem']
            },
            'Answer': {
                'en': ['answer', 'a', 'response', 'reply', 'solution', 'result'],
                'zh': ['ç­”æ¡ˆ', 'å›ç­”', 'è§£ç­”', 'è§£å†³æ–¹æ¡ˆ','å†…å®¹', 'å›å¤', 'a', 'answer'],
                'variations': ['answer', 'a', 'response', 'reply']
            },
            'Image_Link': {
                'en': ['image_link', 'image', 'img_link', 'image_url', 'imagelink', 'picture', 'photo'],
                'zh': ['å›¾ç‰‡é“¾æ¥', 'å›¾ç‰‡', 'ç…§ç‰‡', 'å›¾åƒ', 'é“¾æ¥', 'å›¾ç‰‡åœ°å€', 'å›¾ç‰‡ç½‘å€'],
                'variations': ['image_link', 'image', 'img_link', 'imagelink']
            },
            'Category': {
                'en': ['category', 'cat', 'type', 'class', 'group'],
                'zh': ['ç±»åˆ«', 'åˆ†ç±»', 'ç±»å‹', 'ç§ç±»', 'ç»„åˆ«', 'ç±»'],
                'variations': ['category', 'cat', 'type', 'class']
            },
            'Keywords': {
                'en': ['keywords', 'keyword', 'tags', 'search_terms', 'search terms'],
                'zh': ['å…³é”®è¯', 'å…³é”®å­—', 'æ ‡ç­¾', 'æœç´¢è¯', 'æ£€ç´¢è¯'],
                'variations': ['keywords', 'keyword', 'tags']
            }
        }
        
        # Expected columns
        required_columns = ['SKU', 'Question', 'Answer']
        
        # Create column mapping
        column_mapping = {}
        
        # Convert all column names to lowercase for comparison
        df_columns_lower = {col.lower(): col for col in df.columns}
        
        # Also create mapping without spaces/underscores
        df_columns_normalized = {}
        for col in df.columns:
            normalized = col.lower().replace(' ', '').replace('_', '').replace('-', '')
            df_columns_normalized[normalized] = col
        
        # Map required columns with multilingual support
        for req_col in required_columns:
            found = False
            mapping_config = column_mappings.get(req_col, {})
            
            # Collect all possible names from all languages
            all_possible_names = []
            
            # Add English variations
            all_possible_names.extend(mapping_config.get('en', []))
            # Add Chinese variations  
            all_possible_names.extend(mapping_config.get('zh', []))
            # Add common variations
            all_possible_names.extend(mapping_config.get('variations', []))
            # Add the original column name
            all_possible_names.append(req_col.lower())
            
            # Try exact matching first
            for possible_name in all_possible_names:
                possible_name_lower = possible_name.lower()
                if possible_name_lower in df_columns_lower:
                    column_mapping[req_col] = df_columns_lower[possible_name_lower]
                    found = True
                    logger.info(f"Mapped {req_col} to '{df_columns_lower[possible_name_lower]}' via exact match")
                    break
            
            # Try normalized matching (without spaces/underscores)
            if not found:
                for possible_name in all_possible_names:
                    normalized_name = possible_name.lower().replace(' ', '').replace('_', '').replace('-', '')
                    if normalized_name in df_columns_normalized:
                        column_mapping[req_col] = df_columns_normalized[normalized_name]
                        found = True
                        logger.info(f"Mapped {req_col} to '{df_columns_normalized[normalized_name]}' via normalized match")
                        break
            
            # Try partial matching
            if not found:
                for col_lower, original_col in df_columns_lower.items():
                    for possible_name in all_possible_names:
                        possible_name_lower = possible_name.lower()
                        if (possible_name_lower in col_lower or col_lower in possible_name_lower) and len(possible_name_lower) > 2:
                            column_mapping[req_col] = original_col
                            found = True
                            logger.info(f"Mapped {req_col} to '{original_col}' via partial match with '{possible_name}'")
                            break
                    if found:
                        break
            
            # Special handling for Chinese files - try positional mapping as last resort
            if not found and len(df.columns) >= len(required_columns):
                # For Chinese files, common order is: Index, Content/Question, Image
                # Try to map based on position if we can identify the pattern
                if req_col == 'Question' and len(df.columns) >= 2:
                    # Usually the second column (index 1) contains the main content
                    if 'å†…å®¹' in df.columns[1] or 'é—®é¢˜' in df.columns[1] or not df.columns[1].startswith('Unnamed'):
                        column_mapping[req_col] = df.columns[1]
                        found = True
                        logger.info(f"Mapped {req_col} to '{df.columns[1]}' via positional mapping")
                elif req_col == 'SKU' and len(df.columns) >= 1:
                    # If first column has data (not just Unnamed), use it as SKU
                    if not df.columns[0].startswith('Unnamed') or df.iloc[0, 0] is not None:
                        column_mapping[req_col] = df.columns[0]
                        found = True
                        logger.info(f"Mapped {req_col} to '{df.columns[0]}' via positional mapping")
                elif req_col == 'Answer':
                    # For Answer, we'll generate it from Question if not found
                    pass
            
            if not found and req_col != 'Answer':
                # List available columns for debugging
                available_cols = list(df.columns)
                logger.error(f"Required column '{req_col}' not found. Available columns: {available_cols}")
                
                # Provide helpful suggestions based on what we found
                suggestions = []
                if any('å†…å®¹' in col for col in df.columns):
                    suggestions.append("Found Chinese content column - this might be your Question column")
                if any('å›¾ç‰‡' in col or 'image' in col.lower() for col in df.columns):
                    suggestions.append("Found image-related column")
                    
                suggestion_text = ". Suggestions: " + "; ".join(suggestions) if suggestions else ""
                
                raise ValueError(f"Required column '{req_col}' not found in Excel file. Available columns: {available_cols}{suggestion_text}")
        
        # Map optional columns with the same multilingual approach
        optional_columns = ['Image_Link', 'Category', 'Keywords']
        for opt_col in optional_columns:
            mapping_config = column_mappings.get(opt_col, {})
            
            # Collect all possible names
            all_possible_names = []
            all_possible_names.extend(mapping_config.get('en', []))
            all_possible_names.extend(mapping_config.get('zh', []))
            all_possible_names.extend(mapping_config.get('variations', []))
            all_possible_names.append(opt_col.lower())
            
            # Try to find the column
            for possible_name in all_possible_names:
                possible_name_lower = possible_name.lower()
                if possible_name_lower in df_columns_lower:
                    column_mapping[opt_col] = df_columns_lower[possible_name_lower]
                    logger.info(f"Mapped optional {opt_col} to '{df_columns_lower[possible_name_lower]}'")
                    break
        
        logger.info(f"Final column mapping: {column_mapping}")
        
        qa_entries = []
        
        # Process rows
        for index, row in df.iterrows():
            try:
                # Get SKU
                sku = ""
                if 'SKU' in column_mapping:
                    sku_value = row[column_mapping['SKU']]
                    if pd.notna(sku_value):
                        sku = clean_text(str(sku_value))
                
                # If no SKU found, generate one from row index
                if not sku:
                    sku = f"ITEM_{index + 1:03d}"
                
                # Get Question
                question = ""
                if 'Question' in column_mapping:
                    question_value = row[column_mapping['Question']]
                    if pd.notna(question_value):
                        question = clean_text(str(question_value))
                
                # Skip if no question content
                if not question:
                    logger.warning(f"Skipping row {index + 1}: no question content found")
                    continue
                
                # Get Answer - if not found, use question as answer
                answer = ""
                if 'Answer' in column_mapping:
                    answer_value = row[column_mapping['Answer']]
                    if pd.notna(answer_value):
                        answer = clean_text(str(answer_value))
                
                # If no answer, use question as answer (common in FAQ format)
                if not answer:
                    answer = question
                    logger.info(f"Row {index + 1}: Using question as answer (FAQ format)")
                
                # Extract optional fields
                image_link = ""
                if 'Image_Link' in column_mapping:
                    image_value = row[column_mapping['Image_Link']]
                    if pd.notna(image_value):
                        image_link = clean_text(str(image_value))
                        image_link = clean_image_url(image_link)
                
                category = ""
                if 'Category' in column_mapping:
                    cat_value = row[column_mapping['Category']]
                    if pd.notna(cat_value):
                        category = clean_text(str(cat_value))
                
                keywords = ""
                if 'Keywords' in column_mapping:
                    kw_value = row[column_mapping['Keywords']]
                    if pd.notna(kw_value):
                        keywords = clean_text(str(kw_value))
                
                # If no keywords provided, extract from question and answer
                if not keywords:
                    keywords = extract_keywords_from_text(f"{question} {answer}")
                
                # Determine category if not provided
                if not category:
                    category = determine_category(question, answer)
                
                qa_entry = {
                    'sku': sku,
                    'question': question,
                    'answer': answer,
                    'image_link': image_link,
                    'category': category,
                    'keywords': keywords
                }
                
                qa_entries.append(qa_entry)
                logger.debug(f"Processed row {index + 1}: SKU={sku}, Question={question[:50]}...")
                
            except Exception as e:
                logger.error(f"Error processing row {index + 1}: {str(e)}")
                continue
        
        logger.info(f"Successfully parsed {len(qa_entries)} entries from Excel file")
        return qa_entries
        
    except Exception as e:
        logger.error(f"Error parsing Excel file: {str(e)}")
        raise ValueError(f"Error parsing Excel file: {str(e)}")

# def extract_keywords_from_text(text: str) -> str:
#     """Extract keywords from question and answer for better searching - supports Chinese"""
#     if not text:
#         return ""
    
#     # Detect if text contains Chinese characters
#     def contains_chinese(text):
#         return any('\u4e00' <= char <= '\u9fff' for char in text)
    
#     has_chinese = contains_chinese(text)
    
#     if has_chinese:
#         # Chinese text processing
#         # For Chinese, we extract meaningful phrases and words
#         import re
        
#         # Extract Chinese words (sequences of Chinese characters)
#         chinese_words = re.findall(r'[\u4e00-\u9fff]+', text)
        
#         # Extract English words and numbers
#         english_words = re.findall(r'[a-zA-Z0-9]+', text)
        
#         # Common Chinese stop words
#         chinese_stop_words = {
#             'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 
#             'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½',
#             'è‡ªå·±', 'è¿™', 'é‚£', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'å“ªé‡Œ', 'ä»€ä¹ˆæ—¶å€™',
#             'æ€æ ·', 'ä¸ºäº†', 'å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶å', 'ç°åœ¨', 'å·²ç»', 'å¯ä»¥',
#             'åº”è¯¥', 'éœ€è¦', 'å¿…é¡»', 'èƒ½å¤Ÿ', 'æˆ–è€…', 'è¿˜æ˜¯', 'å¦‚æœ', 'è™½ç„¶', 'é—®', 'ç­”'
#         }
        
#         # Filter Chinese words
#         filtered_chinese = [word for word in chinese_words 
#                            if word not in chinese_stop_words and len(word) > 1]
        
#         # Common English stop words
#         english_stop_words = {
#             'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 
#             'of', 'with', 'by', 'how', 'what', 'where', 'when', 'why', 'is', 'are', 
#             'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 
#             'did', 'will', 'would', 'could', 'should', 'can', 'may', 'might', 'must',
#             'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they'
#         }
        
#         # Filter English words
#         filtered_english = [word.lower() for word in english_words 
#                            if word.lower() not in english_stop_words and len(word) > 2]
        
#         # Combine all keywords
#         all_keywords = filtered_chinese + filtered_english
        
#         # Remove duplicates while preserving order
#         seen = set()
#         unique_keywords = []
#         for keyword in all_keywords:
#             if keyword not in seen:
#                 seen.add(keyword)
#                 unique_keywords.append(keyword)
        
#         return ' '.join(unique_keywords[:20])  # Limit to 20 keywords
    
#     else:
#         # English text processing (original logic)
#         text = text.lower()
        
#         # Remove common stop words
#         stop_words = {
#             'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 
#             'of', 'with', 'by', 'how', 'what', 'where', 'when', 'why', 'is', 'are', 
#             'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 
#             'did', 'will', 'would', 'could', 'should', 'can', 'may', 'might', 'must',
#             'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they'
#         }
        
#         # Extract words and filter
#         words = re.findall(r'\b\w+\b', text)
#         keywords = [word for word in words if word not in stop_words and len(word) > 2]
        
#         # Remove duplicates while preserving order
#         seen = set()
#         unique_keywords = []
#         for keyword in keywords:
#             if keyword not in seen:
#                 seen.add(keyword)
#                 unique_keywords.append(keyword)
        
#         return ' '.join(unique_keywords[:15])  # Limit to 15 keywords for English


def determine_category(question: str, answer: str) -> str:
    """Automatically determine category based on question and answer content - supports Chinese"""
    text = f"{question} {answer}".lower()
    
    # Check if text contains Chinese
    def contains_chinese(text):
        return any('\u4e00' <= char <= '\u9fff' for char in text)
    
    if contains_chinese(text):
        # Chinese category detection
        chinese_category_keywords = {
            'å®‰è£…': ['å®‰è£…', 'å®‰è®¾', 'è£…é…', 'å®‰è£…æ–¹æ³•', 'æ€ä¹ˆè£…', 'å¦‚ä½•å®‰è£…', 'è¿æ¥', 'æ¥çº¿', 'ç„Šæ¥'],
            'æ•…éšœæ’é™¤': ['æ•…éšœ', 'é—®é¢˜', 'ä¸å·¥ä½œ', 'æŸå', 'ä¿®ç†', 'ç»´ä¿®', 'æ’é™¤', 'æ£€æŸ¥', 'ä¸èƒ½', 'æ— æ³•'],
            'ç»´æŠ¤': ['ç»´æŠ¤', 'ä¿å…»', 'æ¸…æ´', 'æ¸…ç†', 'æœåŠ¡', 'æ›´æ¢', 'æ›¿æ¢', 'æ¶¦æ»‘', 'å®šæœŸ'],
            'æ“ä½œ': ['æ“ä½œ', 'ä½¿ç”¨', 'æ€ä¹ˆç”¨', 'å¦‚ä½•ä½¿ç”¨', 'æ§åˆ¶', 'åŠŸèƒ½', 'è¿è¡Œ'],
            'è§„æ ¼': ['è§„æ ¼', 'å°ºå¯¸', 'å¤§å°', 'é‡é‡', 'ææ–™', 'å®¹é‡', 'å‚æ•°', 'æŠ€æœ¯å‚æ•°'],
            'å®‰å…¨': ['å®‰å…¨', 'å±é™©', 'è­¦å‘Š', 'æ³¨æ„', 'é˜²æŠ¤', 'ä¿æŠ¤', 'å®‰å…¨æ€§']
        }
        category_keywords = chinese_category_keywords
    else:
        # English category detection
        category_keywords = {
            'installation': ['install', 'installation', 'setup', 'mount', 'attach', 'connect', 'wiring', 'wire', 'cable', 'welding', 'weld'],
            'troubleshooting': ['problem', 'issue', 'error', 'not working', 'broken', 'fix', 'repair', 'troubleshoot', 'malfunction'],
            'maintenance': ['maintain', 'maintenance', 'clean', 'cleaning', 'service', 'replace', 'replacement', 'lubricate'],
            'operation': ['how to', 'operate', 'operation', 'use', 'usage', 'control', 'controls', 'function'],
            'specifications': ['spec', 'specification', 'dimension', 'size', 'weight', 'material', 'capacity'],
            'safety': ['safety', 'danger', 'warning', 'caution', 'hazard', 'protection', 'secure']
        }
    
    # Count keyword matches for each category
    category_scores = {}
    for category, keywords in category_keywords.items():
        score = sum(1 for keyword in keywords if keyword in text)
        if score > 0:
            category_scores[category] = score
    
    # Return category with highest score, or 'general' if no matches
    if category_scores:
        return max(category_scores, key=category_scores.get)
    else:
        return 'general'

def validate_image_url(url: str) -> bool:
    """Validate if URL is a valid image URL"""
    if not url:
        return False
    
    # Check if URL has image extension
    image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp', '.svg']
    url_lower = url.lower()
    
    return any(url_lower.endswith(ext) for ext in image_extensions) or 'image' in url_lower

def sanitize_filename(filename: str) -> str:
    """Sanitize filename for safe storage"""
    # Remove or replace invalid characters
    filename = re.sub(r'[^\w\s\-_\.]', '', filename)
    filename = re.sub(r'\s+', '_', filename)
    return filename

# Add this function to your existing utils.py file

# Add this function to your existing utils.py file

def format_enhanced_search_results(results: List[Dict]) -> List[Dict]:
    """Enhanced format search results for display with proper score handling"""
    formatted_results = []
    
    for result in results:
        if result['type'] == 'qa_entry':
            entry = result['entry']
            
            # Determine relevance category and proper display for boosted scores
            score = result['score']
            
            # Enhanced display logic for boosted scores
            if score >= 1.0:  # Boosted scores (exact matches)
                relevance_category = 'exact'
                if score >= 1.3:
                    display_score = f"{score:.1f}â˜…â˜…â˜…"  # Triple star for very high boost
                    relevance_text = "ç²¾ç¡®åŒ¹é…"
                elif score >= 1.1:
                    display_score = f"{score:.1f}â˜…â˜…"   # Double star for high boost
                    relevance_text = "é«˜åº¦åŒ¹é…"
                else:
                    display_score = f"{score:.1f}â˜…"    # Single star for boost
                    relevance_text = "åŒ¹é…åŠ å¼º"
            elif score >= 0.7:
                relevance_category = 'high'
                display_score = f"{score:.0%}"
                relevance_text = "é«˜ç›¸å…³"
            elif score >= 0.4:
                relevance_category = 'medium'
                display_score = f"{score:.0%}"
                relevance_text = "ä¸­ç­‰ç›¸å…³"
            else:
                relevance_category = 'low'
                display_score = f"{score:.0%}"
                relevance_text = "å¯èƒ½ç›¸å…³"
            
            formatted_result = {
                'id': entry.id,
                'type': 'qa_entry',
                'sku': entry.sku,
                'question': entry.question,
                'answer': entry.answer,
                'image_link': entry.image_link,
                'category': entry.category or 'general',
                'relevance_score': score,
                'relevance_category': relevance_category,
                'relevance_text': relevance_text,
                'match_type': result['match_type'],
                'match_reason': result['match_reason'],
                'has_image': bool(entry.image_link),
                'display_score': display_score,
                'source_info': result.get('source_info', {}),
                'created_at': getattr(entry, 'created_at', None),
                'updated_at': getattr(entry, 'updated_at', None)
            }
            
            # Add formatted category for display
            if entry.category:
                formatted_result['category_display'] = entry.category.replace('_', ' ').title()
            else:
                formatted_result['category_display'] = 'é€šç”¨'
        
        elif result['type'] == 'document_chunk':
            chunk = result.get('chunk') or result.get('entry')  # Handle both formats
            document = result.get('document')
            
            # Determine relevance for document chunks
            score = result['score']
            if score >= 0.7:
                relevance_category = 'high'
                display_score = f"{score:.0%}"
                relevance_text = "é«˜ç›¸å…³"
            elif score >= 0.4:
                relevance_category = 'medium'
                display_score = f"{score:.0%}"
                relevance_text = "ä¸­ç­‰ç›¸å…³"
            else:
                relevance_category = 'low'
                display_score = f"{score:.0%}"
                relevance_text = "å¯èƒ½ç›¸å…³"
            
            formatted_result = {
                'id': f"chunk_{chunk.id}",
                'type': 'document_chunk',
                'document_id': document.id if document else None,
                'document_title': document.title if document else 'Unknown Document',
                'document_type': document.document_type if document else 'unknown',
                'text_content': result.get('text_content', chunk.text if chunk else ''),
                'page_number': chunk.page_number if chunk else None,
                'chunk_index': chunk.chunk_index if chunk else None,
                'relevance_score': score,
                'relevance_category': relevance_category,
                'relevance_text': relevance_text,
                'match_type': result['match_type'],
                'match_reason': result['match_reason'],
                'display_score': display_score,
                'source_info': result.get('source_info', {}),
                'context_before': chunk.context_before if chunk else None,
                'context_after': chunk.context_after if chunk else None,
                'category': document.category if document else 'general',
                'category_display': 'æ–‡æ¡£å†…å®¹',
                'created_at': document.created_at if document else None
            }
        
        else:
            continue  # Skip unknown types
        
        # Add enhanced match type display
        match_type_display = {
            'exact_sku': 'ç²¾ç¡®SKUåŒ¹é…',
            'partial_sku': 'éƒ¨åˆ†SKUåŒ¹é…', 
            'sku_prefix': 'SKUå‰ç¼€åŒ¹é…',
            'question_match': 'é—®é¢˜åŒ¹é…',
            'semantic': 'è¯­ä¹‰åŒ¹é…',
            'keyword': 'å…³é”®è¯åŒ¹é…'
        }
        
        formatted_result['match_type_display'] = match_type_display.get(
            result['match_type'], result['match_type']
        )
        
        formatted_results.append(formatted_result)
    
    return formatted_results

def generate_excel_template() -> BytesIO:
    """Generate multilingual Excel template for uploading QA data"""
    # Create a new workbook
    wb = openpyxl.Workbook()
    
    # Create English template sheet
    ws_en = wb.active
    ws_en.title = "English_Template"
    
    # English headers
    headers_en = ['SKU', 'Question', 'Answer', 'Image_Link', 'Category', 'Keywords']
    
    # Add English headers with formatting
    for col, header in enumerate(headers_en, 1):
        cell = ws_en.cell(row=1, column=col, value=header)
        from openpyxl.styles import Font, PatternFill
        cell.font = Font(bold=True, color="FFFFFF")
        cell.fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
    
    # Add English descriptions
    descriptions_en = [
        'Product identifier (required)',
        'Question text (required)', 
        'Answer text (required)',
        'Image URL (optional)',
        'Category name (optional)',
        'Search keywords (optional)'
    ]
    
    for col, desc in enumerate(descriptions_en, 1):
        ws_en.cell(row=2, column=col, value=desc)
    
    # Add English sample data
    sample_data_en = [
        ['ABC123', 'How to install the steering wheel?', 'Connect the wiring harness first, then secure with bolts.', 'https://example.com/steering_install.jpg', 'installation', 'steering wheel install wiring'],
        ['ABC123', 'Steering wheel controls not working?', 'Check the wiring connections behind the steering wheel.', 'https://example.com/steering_controls.jpg', 'troubleshooting', 'steering wheel controls troubleshooting'],
        ['XYZ789', 'How to weld the frame?', 'Use MIG welding with proper safety equipment.', 'https://example.com/welding.jpg', 'installation', 'welding frame MIG safety']
    ]
    
    for row, data in enumerate(sample_data_en, 3):
        for col, value in enumerate(data, 1):
            ws_en.cell(row=row, column=col, value=value)
    
    # Create Chinese template sheet
    ws_zh = wb.create_sheet("Chinese_Template")
    
    # Chinese headers
    headers_zh = ['SKU', 'é—®é¢˜', 'ç­”æ¡ˆ', 'å›¾ç‰‡é“¾æ¥', 'ç±»åˆ«', 'å…³é”®è¯']
    
    # Add Chinese headers with formatting
    for col, header in enumerate(headers_zh, 1):
        cell = ws_zh.cell(row=1, column=col, value=header)
        cell.font = Font(bold=True, color="FFFFFF")
        cell.fill = PatternFill(start_color="DC143C", end_color="DC143C", fill_type="solid")
    
    # Add Chinese descriptions
    descriptions_zh = [
        'äº§å“ç¼–å·ï¼ˆå¿…å¡«ï¼‰',
        'é—®é¢˜å†…å®¹ï¼ˆå¿…å¡«ï¼‰', 
        'ç­”æ¡ˆå†…å®¹ï¼ˆå¿…å¡«ï¼‰',
        'å›¾ç‰‡ç½‘å€ï¼ˆå¯é€‰ï¼‰',
        'åˆ†ç±»åç§°ï¼ˆå¯é€‰ï¼‰',
        'æœç´¢å…³é”®è¯ï¼ˆå¯é€‰ï¼‰'
    ]
    
    for col, desc in enumerate(descriptions_zh, 1):
        ws_zh.cell(row=2, column=col, value=desc)
    
    # Add Chinese sample data
    sample_data_zh = [
        ['ABC123', 'å¦‚ä½•å®‰è£…æ–¹å‘ç›˜ï¼Ÿ', 'é¦–å…ˆè¿æ¥çº¿æŸï¼Œç„¶åç”¨èºæ “å›ºå®šã€‚', 'https://example.com/steering_install.jpg', 'å®‰è£…', 'æ–¹å‘ç›˜ å®‰è£… çº¿æŸ'],
        ['ABC123', 'æ–¹å‘ç›˜æ§åˆ¶å™¨ä¸å·¥ä½œï¼Ÿ', 'æ£€æŸ¥æ–¹å‘ç›˜åé¢çš„çº¿è·¯è¿æ¥ã€‚', 'https://example.com/steering_controls.jpg', 'æ•…éšœæ’é™¤', 'æ–¹å‘ç›˜ æ§åˆ¶å™¨ æ•…éšœæ’é™¤'],
        ['XYZ789', 'å¦‚ä½•ç„Šæ¥æ¡†æ¶ï¼Ÿ', 'ä½¿ç”¨MIGç„Šæ¥å¹¶é…å¤‡é€‚å½“çš„å®‰å…¨è®¾å¤‡ã€‚', 'https://example.com/welding.jpg', 'å®‰è£…', 'ç„Šæ¥ æ¡†æ¶ MIG å®‰å…¨']
    ]
    
    for row, data in enumerate(sample_data_zh, 3):
        for col, value in enumerate(data, 1):
            ws_zh.cell(row=row, column=col, value=value)
    
    # Create FAQ format template (for Chinese users who might have single column data)
    ws_faq = wb.create_sheet("FAQ_Format")
    
    # FAQ headers - simple format that matches your uploaded file
    headers_faq = ['SKU', 'å†…å®¹', 'å›¾ç‰‡é“¾æ¥']
    
    for col, header in enumerate(headers_faq, 1):
        cell = ws_faq.cell(row=1, column=col, value=header)
        cell.font = Font(bold=True, color="FFFFFF")
        cell.fill = PatternFill(start_color="228B22", end_color="228B22", fill_type="solid")
    
    # FAQ descriptions
    descriptions_faq = [
        'äº§å“ç¼–å·ï¼ˆå¯ç•™ç©ºï¼Œç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆï¼‰',
        'FAQå†…å®¹ - é—®é¢˜å’Œç­”æ¡ˆï¼ˆå¿…å¡«ï¼‰',
        'ç›¸å…³å›¾ç‰‡é“¾æ¥ï¼ˆå¯é€‰ï¼‰'
    ]
    
    for col, desc in enumerate(descriptions_faq, 1):
        ws_faq.cell(row=2, column=col, value=desc)
    
    # FAQ sample data
    sample_data_faq = [
        ['', 'é—®ï¼šå¦‚ä½•å®‰è£…æ–¹å‘ç›˜ï¼Ÿç­”ï¼šé¦–å…ˆè¿æ¥çº¿æŸï¼Œç„¶åç”¨èºæ “å›ºå®šã€‚', 'https://example.com/steering.jpg'],
        ['', 'é—®ï¼šæ–¹å‘ç›˜æ§åˆ¶å™¨ä¸å·¥ä½œæ€ä¹ˆåŠï¼Ÿç­”ï¼šæ£€æŸ¥æ–¹å‘ç›˜åé¢çš„çº¿è·¯è¿æ¥ã€‚', ''],
        ['', 'é—®ï¼šå¦‚ä½•ç„Šæ¥æ¡†æ¶ï¼Ÿç­”ï¼šä½¿ç”¨MIGç„Šæ¥å¹¶é…å¤‡é€‚å½“çš„å®‰å…¨è®¾å¤‡ã€‚', 'https://example.com/welding.jpg']
    ]
    
    for row, data in enumerate(sample_data_faq, 3):
        for col, value in enumerate(data, 1):
            ws_faq.cell(row=row, column=col, value=value)
    
    # Auto-adjust column widths for all sheets
    for ws in [ws_en, ws_zh, ws_faq]:
        for column in ws.columns:
            max_length = 0
            column_letter = column[0].column_letter
            for cell in column:
                try:
                    if len(str(cell.value)) > max_length:
                        max_length = len(str(cell.value))
                except:
                    pass
            adjusted_width = min(max_length + 2, 50)
            ws.column_dimensions[column_letter].width = adjusted_width
    
    # Create instructions sheet
    instructions_ws = wb.create_sheet("Instructions")
    instructions = [
        ["Multilingual Excel Upload Instructions / å¤šè¯­è¨€Excelä¸Šä¼ è¯´æ˜", ""],
        ["", ""],
        ["English Instructions:", ""],
        ["Required Columns:", "SKU, Question, Answer"],
        ["Optional Columns:", "Image_Link, Category, Keywords"],
        ["", ""],
        ["ä¸­æ–‡è¯´æ˜:", ""],
        ["å¿…éœ€åˆ—:", "SKU, é—®é¢˜, ç­”æ¡ˆ"],
        ["å¯é€‰åˆ—:", "å›¾ç‰‡é“¾æ¥, ç±»åˆ«, å…³é”®è¯"],
        ["", ""],
        ["Supported Column Names / æ”¯æŒçš„åˆ—å:", ""],
        ["SKU:", "SKU, sku, äº§å“ç¼–å·, äº§å“ä»£ç , ç¼–å·, product_id"],
        ["Question:", "Question, é—®é¢˜, å†…å®¹, Q, query, ç–‘é—®"],
        ["Answer:", "Answer, ç­”æ¡ˆ, å›ç­”, A, response, è§£ç­”"],
        ["Image_Link:", "Image_Link, å›¾ç‰‡é“¾æ¥, å›¾ç‰‡, image, img_link"],
        ["Category:", "Category, ç±»åˆ«, åˆ†ç±», type, ç§ç±»"],
        ["Keywords:", "Keywords, å…³é”®è¯, å…³é”®å­—, tags, æ ‡ç­¾"],
        ["", ""],
        ["File Formats / æ–‡ä»¶æ ¼å¼:", ""],
        ["Supported:", ".xlsx, .xls"],
        ["Max Size:", "10MB"],
        ["", ""],
        ["Important Notes / é‡è¦è¯´æ˜:", ""],
        ["â€¢ Column names are case-insensitive / åˆ—åä¸åŒºåˆ†å¤§å°å†™", ""],
        ["â€¢ You can use spaces or underscores / å¯ä»¥ä½¿ç”¨ç©ºæ ¼æˆ–ä¸‹åˆ’çº¿", ""],
        ["â€¢ Empty rows will be skipped / ç©ºè¡Œå°†è¢«è·³è¿‡", ""],
        ["â€¢ If SKU is missing, auto-generated IDs will be used / å¦‚æœç¼ºå°‘SKUï¼Œå°†è‡ªåŠ¨ç”ŸæˆID", ""],
        ["â€¢ For FAQ format: put Q&A in one column / FAQæ ¼å¼ï¼šé—®ç­”æ”¾åœ¨ä¸€åˆ—ä¸­", ""],
        ["", ""],
        ["Template Sheets / æ¨¡æ¿å·¥ä½œè¡¨:", ""],
        ["English_Template:", "Standard English format"],
        ["Chinese_Template:", "Standard Chinese format / æ ‡å‡†ä¸­æ–‡æ ¼å¼"],
        ["FAQ_Format:", "Simplified FAQ format / ç®€åŒ–FAQæ ¼å¼"],
    ]
    
    for row, (key, value) in enumerate(instructions, 1):
        instructions_ws.cell(row=row, column=1, value=key)
        instructions_ws.cell(row=row, column=2, value=value)
        
        # Format headers
        if key and not value and row > 2:
            cell = instructions_ws.cell(row=row, column=1)
            cell.font = Font(bold=True)
    
    # Auto-adjust column widths for instructions
    for column in instructions_ws.columns:
        max_length = 0
        column_letter = column[0].column_letter
        for cell in column:
            try:
                if len(str(cell.value)) > max_length:
                    max_length = len(str(cell.value))
            except:
                pass
        adjusted_width = min(max_length + 2, 80)
        instructions_ws.column_dimensions[column_letter].width = adjusted_width
    
    # Save to BytesIO
    excel_buffer = BytesIO()
    wb.save(excel_buffer)
    excel_buffer.seek(0)
    
    return excel_buffer

def log_user_activity(user_ip: str, action: str, details: str = ""):
    """Log user activity for analytics"""
    logger.info(f"User Activity - IP: {user_ip}, Action: {action}, Details: {details}")

class SearchQueryProcessor:
    """Enhanced query processor optimized for semantic search ONLY"""
    
    def __init__(self):
        # Semantic mappings for better understanding
        self.semantic_mappings = {
            # Common technical terms
            'engine': ['å‘åŠ¨æœº', 'å¼•æ“', 'é©¬è¾¾', 'motor'],
            'transmission': ['å˜é€Ÿç®±', 'ä¼ åŠ¨', 'gearbox'],
            'brake': ['åˆ¹è½¦', 'åˆ¶åŠ¨', 'åˆ¹è½¦ç‰‡'],
            'clutch': ['ç¦»åˆå™¨', 'ç¦»åˆ'],
            'hydraulic': ['æ¶²å‹', 'æ²¹å‹'],
            'electric': ['ç”µåŠ¨', 'ç”µæ°”', 'ç”µåŠ›'],
            'fuel': ['ç‡ƒæ–™', 'ç‡ƒæ²¹', 'æ±½æ²¹', 'gas', 'gasoline', 'diesel'],
            
            # Problem types
            'problem': ['é—®é¢˜', 'æ•…éšœ', 'æ¯›ç—…', 'issue', 'trouble'],
            'error': ['é”™è¯¯', 'å‡ºé”™', 'fault'],
            'broken': ['åäº†', 'æŸå', 'ç ´æŸ', 'damaged'],
            'not working': ['ä¸å·¥ä½œ', 'ä¸è¿è¡Œ', 'å¤±æ•ˆ'],
            
            # Installation terms
            'install': ['å®‰è£…', 'è£…é…', 'ç»„è£…', 'mount', 'setup'],
            'replace': ['æ›´æ¢', 'æ›¿æ¢', 'æ¢', 'change'],
            'repair': ['ä¿®ç†', 'ç»´ä¿®', 'ä¿®å¤', 'fix'],
            'maintenance': ['ä¿å…»', 'ç»´æŠ¤', 'ç»´ä¿®'],
            
            # Common products
            'filter': ['æ»¤å™¨', 'è¿‡æ»¤å™¨', 'strainer'],
            'seal': ['å¯†å°', 'å¯†å°ä»¶', 'gasket'],
            'bearing': ['è½´æ‰¿', 'è½´å¥—'],
            'valve': ['é˜€é—¨', 'é˜€', 'valve'],
            'pump': ['æ³µ', 'æ°´æ³µ', 'æ²¹æ³µ'],
            'sensor': ['ä¼ æ„Ÿå™¨', 'æ„Ÿåº”å™¨'],
            
            # Actions and Procedures
            'check': ['æ£€æŸ¥', 'æŸ¥çœ‹', 'inspect'],
            'test': ['æµ‹è¯•', 'æ£€æµ‹', 'examine'],
            'adjust': ['è°ƒæ•´', 'è°ƒèŠ‚', 'tune'],
            'clean': ['æ¸…æ´', 'æ¸…ç†', 'wash'],
            'lubricate': ['æ¶¦æ»‘', 'åŠ æ²¹', 'oil'],
            'tighten': ['æ‹§ç´§', 'å›ºå®š', 'secure'],
            'loosen': ['æ¾å¼€', 'æ”¾æ¾', 'release'],
            
            # Common questions
            'how': ['å¦‚ä½•', 'æ€ä¹ˆ', 'æ€æ ·'],
            'what': ['ä»€ä¹ˆ', 'å“ªä¸ª'],
            'where': ['å“ªé‡Œ', 'åœ¨å“ª'],
            'when': ['ä»€ä¹ˆæ—¶å€™', 'ä½•æ—¶'],
            'why': ['ä¸ºä»€ä¹ˆ', 'ä¸ºä½•'],
            'which': ['å“ªä¸ª', 'å“ªä¸€ä¸ª'],
        }
        
        # Common stop words for filtering
        self.chinese_stops = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 
            'è¿™', 'é‚£', 'ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 
            'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'ç„¶å', 'ç°åœ¨', 'å·²ç»', 'å¯ä»¥', 'åº”è¯¥',
            'éœ€è¦', 'å¿…é¡»', 'èƒ½å¤Ÿ', 'æˆ–è€…', 'è¿˜æ˜¯', 'å¦‚æœ', 'è™½ç„¶', 'å› ä¸º', 'æ‰€ä»¥',
            'ä½†æ˜¯', 'é—®', 'ç­”'
        }
        
        self.english_stops = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 
            'of', 'with', 'by', 'from', 'as', 'is', 'are', 'was', 'were', 'be', 
            'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 
            'would', 'could', 'should', 'can', 'may', 'might', 'must', 'this', 
            'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they'
        }
    
    def clean_query(self, query: str) -> str:
        """Clean and normalize query while preserving ALL content including potential SKUs"""
        # Remove extra whitespace
        query = re.sub(r'\s+', ' ', query).strip()
        
        # Remove special characters but keep basic punctuation, Chinese characters, AND alphanumeric
        # This preserves SKU-like patterns as regular search terms
        query = re.sub(r'[^\w\s\-_.,!?ï¼Œã€‚ï¼ï¼Ÿ\u4e00-\u9fff]', '', query)
        
        return query
    
    def extract_semantic_terms(self, query: str) -> List[str]:
        """Extract semantic terms WITHOUT removing SKU patterns - treat everything as semantic"""
        cleaned_query = query.strip()
        
        # NO SKU REMOVAL - treat potential SKUs as regular search terms
        # This is the key change - we don't filter out anything that looks like a SKU
        
        # Extract both Chinese and English terms, including alphanumeric patterns
        chinese_chars = re.findall(r'[\u4e00-\u9fff]+', cleaned_query)
        english_words = re.findall(r'[a-zA-Z]{1,}', cleaned_query.lower())  # Allow single chars
        alphanumeric = re.findall(r'[a-zA-Z]*\d+[a-zA-Z]*|\d*[a-zA-Z]+\d*', cleaned_query)  # Include alphanumeric patterns
        
        # Filter terms but keep more content
        chinese_terms = [term for term in chinese_chars 
                        if len(term) >= 1 and term not in self.chinese_stops]
        english_terms = [term for term in english_words 
                        if len(term) >= 1 and term not in self.english_stops]  # Reduced minimum length
        alphanumeric_terms = [term.lower() for term in alphanumeric if len(term) >= 1]
        
        # Start with original terms INCLUDING potential SKUs
        expanded_terms = chinese_terms + english_terms + alphanumeric_terms
        
        # Expand using semantic mappings
        for english_term in english_terms:
            if english_term in self.semantic_mappings:
                expanded_terms.extend(self.semantic_mappings[english_term])
        
        # Reverse lookup for Chinese terms
        for chinese_term in chinese_terms:
            for english_key, translations in self.semantic_mappings.items():
                if chinese_term in translations:
                    expanded_terms.append(english_key)
                    # Add other translations too
                    expanded_terms.extend([t for t in translations if t != chinese_term])
        
        # Remove duplicates while preserving order
        seen = set()
        unique_terms = []
        for term in expanded_terms:
            if term not in seen:
                seen.add(term)
                unique_terms.append(term)
        
        # Log the expansion for debugging
        logger.info(f"ğŸ” Semantic terms extracted: {unique_terms[:10]}...")  # Show first 10
        
        return unique_terms[:25]  # Increased limit for better semantic context
    
    def get_query_intent(self, query: str) -> str:
        """Detect query intent for better RAG prompting"""
        query_lower = query.lower()
        
        if any(term in query_lower for term in ['how', 'install', 'å®‰è£…', 'å¦‚ä½•', 'setup']):
            return 'installation'
        elif any(term in query_lower for term in ['problem', 'error', 'é—®é¢˜', 'æ•…éšœ', 'troubleshoot', 'fix']):
            return 'troubleshooting'
        elif any(term in query_lower for term in ['what', 'which', 'ä»€ä¹ˆ', 'å“ªä¸ª', 'specifications', 'spec']):
            return 'information'
        elif any(term in query_lower for term in ['replace', 'repair', 'æ›´æ¢', 'ä¿®ç†', 'maintenance']):
            return 'maintenance'
        else:
            return 'general'